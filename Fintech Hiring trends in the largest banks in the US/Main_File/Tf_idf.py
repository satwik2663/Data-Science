# -*- coding: utf-8 -*-
"""TFIDFKeywords

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qEeEjurrkAzbP-KNxGbSOfW3ROuNA240
"""

from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
import math
from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.converter import TextConverter
from pdfminer.layout import LAParams
from pdfminer.pdfpage import PDFPage
from io import StringIO
import pandas as pd
import xlrd
import csv

import nltk

nltk.download('stopwords')

merged_text = None


def convert_pdf_to_txt(path):
    rsrcmgr = PDFResourceManager()
    retstr = StringIO()
    codec = 'utf-8'
    laparams = LAParams()
    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)
    fp = open(path, 'rb')
    interpreter = PDFPageInterpreter(rsrcmgr, device)
    maxpages = 0
    caching = True
    pagenos = set()

    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, caching=caching, check_extractable=True):
        interpreter.process_page(page)

    text = retstr.getvalue()
    # print(text)
    global merged_text
    merged_text = text
    fp.close()
    device.close()
    retstr.close()
    return text

def remove_string_special_characters(s):
    print(s)

    stripped = re.sub('[^\w\s]', ' ', s)
    stripped = re.sub('_', ' ', stripped)
    stripped = re.sub(',', ' ', stripped)
    stripped = re.sub('\s+', ' ', stripped)
    stripped = stripped.strip()

    return stripped

merged_text = open(r'../Document_Reports/Combined_Reports.txt', 'r', encoding='utf-8').read()
removed_sp_char = remove_string_special_characters(merged_text)

removed_sp_char_lowered = removed_sp_char.lower()
stop_words = set(stopwords.words('english'))
filtered_sentence = []
word_tokens = word_tokenize(removed_sp_char_lowered)
for w in word_tokens:
    if w not in stop_words:
        filtered_sentence.append(w)

print(filtered_sentence)

filtered_sentence_string = ' '.join(filtered_sentence)

tfidf = TfidfVectorizer()

response = tfidf.fit_transform([filtered_sentence_string])

feature_names = tfidf.get_feature_names()
tfidfwithscores = []
for col in response.nonzero()[1]:
    print(feature_names[col], ' - ', response[0, col])
    tfidfwithscores.append([feature_names[col], response[0, col]])

#df = pd.DataFrame([feature_names, response])

dic = {}
a =[]
b=[]
for i,j in tfidfwithscores:
     a.append(i)
     b.append(j)
dic['keyWords'] = a
tfidfwithscores = pd.DataFrame(dic,index=a)

#tfidfwithscoressorted = tfidfwithscores.sort_values(by=[1], ascending=False)

tfidfwithscores.to_excel(r'../Documents_Top_100/TF_IDF.xlsx')

def csv_from_excel():
    wb = xlrd.open_workbook(r'../Documents_Top_100/TF_IDF.xlsx')
    sh = wb.sheet_by_name('Sheet1')
    your_csv_file = open(r'../Documents_Top_100/TF_IDF.csv', 'w', newline='')
    wr = csv.writer(your_csv_file, quoting=csv.QUOTE_ALL)

    for rownum in range(sh.nrows):
        wr.writerow(sh.row_values(rownum))

    your_csv_file.close()
    print('=========== Word Count List : CSV File Printed Successfully ===========')


csv_from_excel()